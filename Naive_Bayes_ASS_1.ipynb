{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3b2f03-5368-4727-a99d-d20aed52766f",
   "metadata": {},
   "source": [
    "# Naive Bayes Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b619358-638e-4c57-ba54-8ef9f10ba5ea",
   "metadata": {},
   "source": [
    "Q 1 ANS:- \n",
    "\n",
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes how to update the probability of a hypothesis or event based on new evidence or information. The theorem mathematically relates the conditional probability of an event or hypothesis given prior knowledge to the conditional probability of the same event given additional evidence.\n",
    "\n",
    "In its simplest form, Bayes' theorem can be stated as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "- P(A|B) represents the probability of event A occurring given that event B has occurred.\n",
    "- P(B|A) represents the probability of event B occurring given that event A has occurred.\n",
    "- P(A) and P(B) are the probabilities of events A and B occurring independently of each other.\n",
    "\n",
    "In other words, Bayes' theorem allows us to calculate the probability of event A happening, given that event B has occurred, by considering the prior probability of event A and the likelihood of event B occurring given event A.\n",
    "\n",
    "Bayes' theorem is particularly useful when dealing with uncertain or incomplete information, as it enables the revision of probabilities as new evidence becomes available. It has applications in various fields, including statistics, machine learning, data analysis, and artificial intelligence, where it is used for tasks such as classification, decision-making, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8b505-6445-4a76-9c36-802db31256eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7160104b-b3fb-45f3-bbb7-4424beead920",
   "metadata": {},
   "source": [
    "Q 2 ANS:-\n",
    "\n",
    "The formula for Bayes' theorem as:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "- P(A|B) is the probability of event A occurring given that event B has occurred.\n",
    "- P(B|A) is the probability of event B occurring given that event A has occurred.\n",
    "- P(A) is the prior probability of event A.\n",
    "- P(B) is the prior probability of event B.\n",
    "\n",
    "In words, Bayes' theorem states that the probability of event A given event B is equal to the probability of event B given event A multiplied by the prior probability of event A, divided by the prior probability of event B.\n",
    "\n",
    "This formula allows for the updating of probabilities based on new information. By incorporating the likelihood of observing event B given event A, along with the prior probabilities, Bayes' theorem provides a way to revise and refine our estimates of the probabilities involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08e96c-6884-43f3-8b71-384a69101a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7797b40-7a34-4b4d-9a1f-bbe45aec6d82",
   "metadata": {},
   "source": [
    "Q 3 ANS:-\n",
    "\n",
    "Bayes' theorem is used in various practical applications across different fields. Here are a few examples of how Bayes' theorem is applied in practice:\n",
    "\n",
    "1. Medical Diagnosis: Bayes' theorem plays a crucial role in medical diagnosis. Doctors use prior knowledge about the prevalence of certain diseases, along with specific test results, to calculate the probability of a patient having a particular condition. By updating the probabilities based on test outcomes, Bayes' theorem helps refine the diagnosis.\n",
    "\n",
    "2. Spam Filtering: Email spam filters often employ Bayesian techniques. Initially, the filter is trained on a set of known spam and non-spam emails to estimate the probabilities of certain words or phrases occurring in spam or legitimate emails. When new emails arrive, the filter applies Bayes' theorem to calculate the probability of an email being spam based on the occurrence of certain words.\n",
    "\n",
    "3. Document Classification: Bayes' theorem is used in text classification tasks such as sentiment analysis or topic identification. By training a classifier on labeled documents, the probabilities of words occurring in each class are estimated. When classifying new documents, Bayes' theorem is used to calculate the probability of a document belonging to a specific class based on the observed word frequencies.\n",
    "\n",
    "4. Risk Assessment: Bayes' theorem is applied in risk assessment and decision-making processes. Prior probabilities, combined with new evidence or data, are used to update the probabilities of certain outcomes occurring. This enables decision-makers to incorporate new information and adjust their assessments of risks and potential outcomes.\n",
    "\n",
    "5. Machine Learning: Bayesian inference is a technique used in machine learning algorithms. It involves updating the model's parameters based on observed data. Bayesian models use Bayes' theorem to calculate the posterior probability of the model parameters given the data, allowing for more robust and accurate predictions.\n",
    "\n",
    "These are just a few examples of how Bayes' theorem is used in practice. Its versatility and ability to update probabilities based on new information make it a valuable tool in probabilistic reasoning, statistical analysis, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f037f05-5d26-459e-a7ef-45a13f9e3238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f222172-8d4a-4d6d-bfe0-2767c53c730a",
   "metadata": {},
   "source": [
    "Q 4 ANS:-\n",
    "\n",
    "Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the definition of conditional probability.\n",
    "\n",
    "Conditional probability is the probability of an event A occurring given that another event B has already occurred, and it is denoted as P(A|B). It is calculated as:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "where P(A ∩ B) represents the probability of both events A and B occurring together, and P(B) is the probability of event B occurring.\n",
    "\n",
    "Bayes' theorem builds upon the concept of conditional probability and allows for the inversion of the conditional probability relationship. It states that:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Here, P(B|A) represents the probability of event B occurring given that event A has occurred. P(A) is the prior probability of event A, and P(B) is the prior probability of event B.\n",
    "\n",
    "In other words, Bayes' theorem enables us to calculate the conditional probability of event A given event B by incorporating the likelihood of event B given event A, along with the prior probabilities of events A and B.\n",
    "\n",
    "Bayes' theorem provides a powerful framework for updating probabilities based on new evidence or information, allowing us to revise our beliefs or estimates in light of observed data. It is a fundamental concept in probability theory and forms the basis for Bayesian inference and statistical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaf78f-f0b4-412b-9407-91b0c425d670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55ff7fcf-4336-4a8e-9531-e982537228a0",
   "metadata": {},
   "source": [
    "Q 5 ANS:-\n",
    "\n",
    "When choosing a type of Naive Bayes classifier for a specific problem, it is important to consider the characteristics of the data and the assumptions made by each variant. The three common types of Naive Bayes classifiers are Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's a general guideline for selecting the appropriate type:\n",
    "\n",
    "1. Gaussian Naive Bayes:\n",
    "   - Assumption: Assumes continuous features follow a Gaussian (normal) distribution.\n",
    "   - Suitable for: Problems with continuous numerical features.\n",
    "   - Example applications: Classification of data with real-valued features, such as predicting the price of a house based on its attributes.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "   - Assumption: Assumes features follow a multinomial distribution (usually for discrete or count-based features).\n",
    "   - Suitable for: Problems with discrete features representing occurrence counts, such as text classification.\n",
    "   - Example applications: Text classification (document categorization, sentiment analysis) based on word frequency or presence in the document.\n",
    "\n",
    "3. Bernoulli Naive Bayes:\n",
    "   - Assumption: Assumes features are binary (e.g., presence or absence of a feature).\n",
    "   - Suitable for: Problems with binary or Boolean features.\n",
    "   - Example applications: Spam detection based on the presence or absence of specific words in an email, sentiment analysis using binary feature indicators.\n",
    "\n",
    "The choice of Naive Bayes classifier depends on the nature of your data and the assumptions that align with it. However, it's important to note that Naive Bayes classifiers are known for their simplicity and computational efficiency, rather than being the most sophisticated models. If the assumptions of any specific variant do not hold well for your data, more advanced models like decision trees, support vector machines (SVMs), or neural networks might be considered.\n",
    "\n",
    "It's also a good practice to evaluate and compare the performance of different classifiers on your specific problem using appropriate evaluation metrics and techniques such as cross-validation. This will help you determine which classifier performs better in terms of accuracy, precision, recall, or other relevant metrics for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa972f26-82ef-4d7b-bdae-46621a134ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7e0e1f4-cad4-4ae6-89c4-d73aa191ddf1",
   "metadata": {},
   "source": [
    "Q 6 ANS:-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27837d35-74aa-4e7a-a93b-d8c2ce8f9d05",
   "metadata": {},
   "source": [
    "To predict the class using Naive Bayes, we need to calculate the likelihood probabilities and then apply Bayes' theorem. Let's calculate the probabilities step by step:\n",
    "\n",
    "1. Calculate the prior probabilities:\n",
    "Since equal prior probabilities are assumed for each class, we have P(A) = P(B) = 0.5.\n",
    "\n",
    "2. Calculate the likelihood probabilities:\n",
    "To calculate the likelihood probabilities, we need to count the occurrences of each feature value given each class.\n",
    "\n",
    "For Class A:\n",
    "- P(X1=3|A) = 4/13\n",
    "- P(X2=4|A) = 3/13\n",
    "\n",
    "For Class B:\n",
    "- P(X1=3|B) = 1/9\n",
    "- P(X2=4|B) = 3/9\n",
    "\n",
    "3. Apply Bayes' theorem:\n",
    "Now, we can apply Bayes' theorem to calculate the posterior probabilities for each class:\n",
    "\n",
    "For Class A:\n",
    "P(A|X1=3, X2=4) = (P(X1=3|A) * P(X2=4|A) * P(A)) / P(X1=3, X2=4)\n",
    "               = (4/13 * 3/13 * 0.5) / P(X1=3, X2=4)\n",
    "\n",
    "For Class B:\n",
    "P(B|X1=3, X2=4) = (P(X1=3|B) * P(X2=4|B) * P(B)) / P(X1=3, X2=4)\n",
    "               = (1/9 * 3/9 * 0.5) / P(X1=3, X2=4)\n",
    "\n",
    "4. Calculate the joint probability:\n",
    "To calculate the joint probability, we sum the probabilities of all instances with X1=3 and X2=4.\n",
    "\n",
    "P(X1=3, X2=4) = P(X1=3, X2=4|A) * P(A) + P(X1=3, X2=4|B) * P(B)\n",
    "              = (4/13 * 3/13 * 0.5) + (1/9 * 3/9 * 0.5)\n",
    "\n",
    "5. Calculate the posterior probabilities:\n",
    "Now, we can substitute the values into the formulas for P(A|X1=3, X2=4) and P(B|X1=3, X2=4) using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3, X2=4) = (4/13 * 3/13 * 0.5) / P(X1=3, X2=4)\n",
    "P(B|X1=3, X2=4) = (1/9 * 3/9 * 0.5) / P(X1=3, X2=4)\n",
    "\n",
    "6. Compare the probabilities:\n",
    "Compare P(A|X1=3, X2=4) and P(B|X1=3, X2=4) to determine which class has the higher probability. The class with the higher probability is the predicted class.\n",
    "\n",
    "After calculating the probabilities and substituting the values, you can determine the class that Naive Bayes would predict the new instance to belong to based on the higher posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46761ef-ac96-4407-8f1d-8784313f249d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
